{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m HeartDiseaseDataset(X_train, y_train)\n\u001b[1;32m     79\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m HeartDiseaseDataset(X_val, y_val)\n\u001b[0;32m---> 80\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m Data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from ucimlrepo import fetch_ucirepo  # Ensure this package is installed\n",
    "\n",
    "# Load Cleveland Heart Disease dataset\n",
    "heart_disease = fetch_ucirepo(id=45)  # UCI ML Repository ID for Cleveland Dataset\n",
    "\n",
    "# Combine features and target\n",
    "df = pd.concat([heart_disease.data.features, heart_disease.data.targets], axis=1)\n",
    "\n",
    "# Assign column names\n",
    "df.columns = [\n",
    "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', \n",
    "    'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'\n",
    "]\n",
    "\n",
    "# Simplify 'num' column to binary classification\n",
    "df['target'] = df['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df.drop('num', axis=1, inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "df.replace(['?', ''], np.nan, inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df.drop('target', axis=1).values)\n",
    "y = df['target'].values\n",
    "\n",
    "# Define PyTorch Dataset\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# Define Logistic Regression-inspired Neural Network\n",
    "class LRInspiredNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LRInspiredNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Cross-validation setup\n",
    "k_folds = 5\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Collect fold results and structure them for averaging\n",
    "all_folds_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Reset best model state at the start of each fold\n",
    "    best_model_state = None\n",
    "\n",
    "    # Split data for this fold\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = HeartDiseaseDataset(X_train, y_train)\n",
    "    val_dataset = HeartDiseaseDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = LRInspiredNN(input_size=X.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_batch = y_batch.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_batch = y_batch.float()\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()  # Save best model state for current fold\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Load the best model state for this fold, if available\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_prob = model(torch.tensor(X_val, dtype=torch.float32)).squeeze().numpy()\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    # Generate classification report for this fold and append to results\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    all_folds_results.append(report)\n",
    "\n",
    "    # Plot training and validation loss for each fold\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training and Validation Loss - Fold {fold + 1}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Convert all_folds_results into a DataFrame\n",
    "fold_results_df = pd.json_normalize(all_folds_results)\n",
    "\n",
    "# Calculate the average metrics across all folds and display rounded results\n",
    "average_report = fold_results_df.mean().round(2)\n",
    "print(\"\\nAverage Cross-Validation Results:\")\n",
    "print(average_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
